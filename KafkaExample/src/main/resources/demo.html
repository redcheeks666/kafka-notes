<!--&lt;!&ndash;-->
<!-- Licensed to the Apache Software Foundation (ASF) under one or more-->
<!-- contributor license agreements.  See the NOTICE file distributed with-->
<!-- this work for additional information regarding copyright ownership.-->
<!-- The ASF licenses this file to You under the Apache License, Version 2.0-->
<!-- (the "License"); you may not use this file except in compliance with-->
<!-- the License.  You may obtain a copy of the License at-->

<!--    http://www.apache.org/licenses/LICENSE-2.0-->

<!-- Unless required by applicable law or agreed to in writing, software-->
<!-- distributed under the License is distributed on an "AS IS" BASIS,-->
<!-- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.-->
<!-- See the License for the specific language governing permissions and-->
<!-- limitations under the License.-->
<!--&ndash;&gt;-->


<!--    <h3><a id="networklayer" href="#networklayer">5.1 Network Layer</a></h3>-->
<!--    <p>-->
<!--        The network layer is a fairly straight-forward NIO server, and will not be described in great detail. The sendfile implementation is done by giving the <code>MessageSet</code> interface a <code>writeTo</code> method. This allows the file-backed message set to use the more efficient <code>transferTo</code> implementation instead of an in-process buffered write. The threading model is a single acceptor thread and <i>N</i> processor threads which handle a fixed number of connections each. This design has been pretty thoroughly tested <a href="http://sna-projects.com/blog/2009/08/introducing-the-nio-socketserver-implementation">elsewhere</a> and found to be simple to implement and fast. The protocol is kept quite simple to allow for future implementation of clients in other languages.-->
<!--    </p>-->
<!--    <h3><a id="messages" href="#messages">5.2 Messages</a></h3>-->
<!--    <p>-->
<!--        Messages consist of a variable-length header, a variable length opaque key byte array and a variable length opaque value byte array. The format of the header is described in the following section.-->
<!--        Leaving the key and value opaque is the right decision: there is a great deal of progress being made on serialization libraries right now, and any particular choice is unlikely to be right for all uses. Needless to say a particular application using Kafka would likely mandate a particular serialization type as part of its usage. The <code>RecordBatch</code> interface is simply an iterator over messages with specialized methods for bulk reading and writing to an NIO <code>Channel</code>.</p>-->

<!--    <h3><a id="messageformat" href="#messageformat">5.3 Message Format</a></h3>-->
<!--    <p>-->
<!--        Messages (aka Records) are always written in batches. The technical term for a batch of messages is a record batch, and a record batch contains one or more records. In the degenerate case, we could have a record batch containing a single record.-->
<!--        Record batches and records have their own headers. The format of each is described below. </p>-->

<!--    <h4><a id="recordbatch" href="#recordbatch">5.3.1 Record Batch</a></h4>-->
<!--    <p> The following is the on-disk format of a RecordBatch. </p>-->
<!--    <p><pre class="brush: java;">-->
<!--		baseOffset: int64-->
<!--		batchLength: int32-->
<!--		partitionLeaderEpoch: int32-->
<!--		magic: int8 (current magic value is 2)-->
<!--		crc: int32-->
<!--		attributes: int16-->
<!--			bit 0~2:-->
<!--				0: no compression-->
<!--				1: gzip-->
<!--				2: snappy-->
<!--				3: lz4-->
<!--				4: zstd-->
<!--			bit 3: timestampType-->
<!--			bit 4: isTransactional (0 means not transactional)-->
<!--			bit 5: isControlBatch (0 means not a control batch)-->
<!--			bit 6~15: unused-->
<!--		lastOffsetDelta: int32-->
<!--		firstTimestamp: int64-->
<!--		maxTimestamp: int64-->
<!--		producerId: int64-->
<!--		producerEpoch: int16-->
<!--		baseSequence: int32-->
<!--		records: [Record]-->
<!--	</pre></p>-->
<!--    <p> Note that when compression is enabled, the compressed record data is serialized directly following the count of the number of records. </p>-->

<!--    <p>The CRC covers the data from the attributes to the end of the batch (i.e. all the bytes that follow the CRC). It is located after the magic byte, which-->
<!--        means that clients must parse the magic byte before deciding how to interpret the bytes between the batch length and the magic byte. The partition leader-->
<!--        epoch field is not included in the CRC computation to avoid the need to recompute the CRC when this field is assigned for every batch that is received by-->
<!--        the broker. The CRC-32C (Castagnoli) polynomial is used for the computation.</p>-->

<!--    <p>On compaction: unlike the older message formats, magic v2 and above preserves the first and last offset/sequence numbers from the original batch when the log is cleaned. This is required in order to be able to restore the-->
<!--        producer's state when the log is reloaded. If we did not retain the last sequence number, for example, then after a partition leader failure, the producer might see an OutOfSequence error. The base sequence number must-->
<!--        be preserved for duplicate checking (the broker checks incoming Produce requests for duplicates by verifying that the first and last sequence numbers of the incoming batch match the last from that producer). As a result,-->
<!--        it is possible to have empty batches in the log when all the records in the batch are cleaned but batch is still retained in order to preserve a producer's last sequence number. One oddity here is that the baseTimestamp-->
<!--        field is not preserved during compaction, so it will change if the first record in the batch is compacted away.</p>-->

<!--    <h5><a id="controlbatch" href="#controlbatch">5.3.1.1 Control Batches</a></h5>-->
<!--    <p>A control batch contains a single record called the control record. Control records should not be passed on to applications. Instead, they are used by consumers to filter out aborted transactional messages.</p>-->
<!--    <p> The key of a control record conforms to the following schema: </p>-->
<!--    <p><pre class="brush: java">-->
<!--       version: int16 (current version is 0)-->
<!--       type: int16 (0 indicates an abort marker, 1 indicates a commit)-->
<!--    </pre></p>-->
<!--    <p>The schema for the value of a control record is dependent on the type. The value is opaque to clients.</p>-->

<!--    <h4><a id="record" href="#record">5.3.2 Record</a></h4>-->
<!--    <p>Record level headers were introduced in Kafka 0.11.0. The on-disk format of a record with Headers is delineated below. </p>-->
<!--    <p><pre class="brush: java;">-->
<!--		length: varint-->
<!--		attributes: int8-->
<!--			bit 0~7: unused-->
<!--		timestampDelta: varint-->
<!--		offsetDelta: varint-->
<!--		keyLength: varint-->
<!--		key: byte[]-->
<!--		valueLen: varint-->
<!--		value: byte[]-->
<!--		Headers => [Header]-->
<!--	</pre></p>-->
<!--    <h5><a id="recordheader" href="#recordheader">5.3.2.1 Record Header</a></h5>-->
<!--    <p><pre class="brush: java;">-->
<!--		headerKeyLength: varint-->
<!--		headerKey: String-->
<!--		headerValueLength: varint-->
<!--		Value: byte[]-->
<!--	</pre></p>-->
<!--    <p>We use the same varint encoding as Protobuf. More information on the latter can be found <a href="https://developers.google.com/protocol-buffers/docs/encoding#varints">here</a>. The count of headers in a record-->
<!--        is also encoded as a varint.</p>-->

<!--    <h4><a id="messageset" href="#messageset">5.3.3 Old Message Format</a></h4>-->
<!--    <p>-->
<!--        Prior to Kafka 0.11, messages were transferred and stored in <i>message sets</i>. In a message set, each message has its own metadata. Note that although message sets are represented as an array,-->
<!--        they are not preceded by an int32 array size like other array elements in the protocol.-->
<!--    </p>-->

<!--    <b>Message Set:</b><br>-->
<!--    <p><pre class="brush: java;">-->
<!--    MessageSet (Version: 0) => [offset message_size message]-->
<!--        offset => INT64-->
<!--        message_size => INT32-->
<!--        message => crc magic_byte attributes key value-->
<!--            crc => INT32-->
<!--            magic_byte => INT8-->
<!--            attributes => INT8-->
<!--                bit 0~2:-->
<!--                    0: no compression-->
<!--                    1: gzip-->
<!--                    2: snappy-->
<!--                bit 3~7: unused-->
<!--            key => BYTES-->
<!--            value => BYTES-->
<!--    </pre></p>-->
<!--    <p><pre class="brush: java;">-->
<!--    MessageSet (Version: 1) => [offset message_size message]-->
<!--        offset => INT64-->
<!--        message_size => INT32-->
<!--        message => crc magic_byte attributes key value-->
<!--            crc => INT32-->
<!--            magic_byte => INT8-->
<!--            attributes => INT8-->
<!--                bit 0~2:-->
<!--                    0: no compression-->
<!--                    1: gzip-->
<!--                    2: snappy-->
<!--                    3: lz4-->
<!--                bit 3: timestampType-->
<!--                    0: create time-->
<!--                    1: log append time-->
<!--                bit 4~7: unused-->
<!--            timestamp =>INT64-->
<!--            key => BYTES-->
<!--            value => BYTES-->
<!--    </pre></p>-->
<!--    <p>-->
<!--        In versions prior to Kafka 0.10, the only supported message format version (which is indicated in the magic value) was 0. Message format version 1 was introduced with timestamp support in version 0.10.-->
<!--    <ul>-->
<!--        <li>Similarly to version 2 above, the lowest bits of attributes represent the compression type.</li>-->
<!--        <li>In version 1, the producer should always set the timestamp type bit to 0. If the topic is configured to use log append time,-->
<!--            (through either broker level config log.message.timestamp.type = LogAppendTime or topic level config message.timestamp.type = LogAppendTime),-->
<!--            the broker will overwrite the timestamp type and the timestamp in the message set.</li>-->
<!--        <li>The highest bits of attributes must be set to 0.</li>-->
<!--    </ul>-->
<!--    </p>-->
<!--    <p>In message format versions 0 and 1 Kafka supports recursive messages to enable compression. In this case the message's attributes must be set-->
<!--        to indicate one of the compression types and the value field will contain a message set compressed with that type. We often refer-->
<!--        to the nested messages as "inner messages" and the wrapping message as the "outer message." Note that the key should be null-->
<!--        for the outer message and its offset will be the offset of the last inner message.-->
<!--    </p>-->
<!--    <p>When receiving recursive version 0 messages, the broker decompresses them and each inner message is assigned an offset individually.-->
<!--        In version 1, to avoid server side re-compression, only the wrapper message will be assigned an offset. The inner messages-->
<!--        will have relative offsets. The absolute offset can be computed using the offset from the outer message, which corresponds-->
<!--        to the offset assigned to the last inner message.-->
<!--    </p>-->

<!--    <p>The crc field contains the CRC32 (and not CRC-32C) of the subsequent message bytes (i.e. from magic byte to the value).</p>-->

<!--    <h3><a id="log" href="#log">5.4 Log</a></h3>-->
<!--    <p>-->
<!--        A log for a topic named "my_topic" with two partitions consists of two directories (namely <code>my_topic_0</code> and <code>my_topic_1</code>) populated with data files containing the messages for that topic. The format of the log files is a sequence of "log entries""; each log entry is a 4 byte integer <i>N</i> storing the message length which is followed by the <i>N</i> message bytes. Each message is uniquely identified by a 64-bit integer <i>offset</i> giving the byte position of the start of this message in the stream of all messages ever sent to that topic on that partition. The on-disk format of each message is given below. Each log file is named with the offset of the first message it contains. So the first file created will be 00000000000.kafka, and each additional file will have an integer name roughly <i>S</i> bytes from the previous file where <i>S</i> is the max log file size given in the configuration.-->
<!--    </p>-->
<!--    <p>-->
<!--        The exact binary format for records is versioned and maintained as a standard interface so record batches can be transferred between producer, broker, and client without recopying or conversion when desirable. The previous section included details about the on-disk format of records.</p>-->
<!--    </p>-->
<!--    <p>-->
<!--        The use of the message offset as the message id is unusual. Our original idea was to use a GUID generated by the producer, and maintain a mapping from GUID to offset on each broker. But since a consumer must maintain an ID for each server, the global uniqueness of the GUID provides no value. Furthermore, the complexity of maintaining the mapping from a random id to an offset requires a heavy weight index structure which must be synchronized with disk, essentially requiring a full persistent random-access data structure. Thus to simplify the lookup structure we decided to use a simple per-partition atomic counter which could be coupled with the partition id and node id to uniquely identify a message; this makes the lookup structure simpler, though multiple seeks per consumer request are still likely. However once we settled on a counter, the jump to directly using the offset seemed natural&mdash;both after all are monotonically increasing integers unique to a partition. Since the offset is hidden from the consumer API this decision is ultimately an implementation detail and we went with the more efficient approach.-->
<!--    </p>-->
<!--    <img class="centered" src="/{{version}}/images/kafka_log.png">-->
<!--    <h4><a id="impl_writes" href="#impl_writes">Writes</a></h4>-->
<!--    <p>-->
<!--        The log allows serial appends which always go to the last file. This file is rolled over to a fresh file when it reaches a configurable size (say 1GB). The log takes two configuration parameters: <i>M</i>, which gives the number of messages to write before forcing the OS to flush the file to disk, and <i>S</i>, which gives a number of seconds after which a flush is forced. This gives a durability guarantee of losing at most <i>M</i> messages or <i>S</i> seconds of data in the event of a system crash.-->
<!--    </p>-->
<!--    <h4><a id="impl_reads" href="#impl_reads">Reads</a></h4>-->
<!--    <p>-->
<!--        Reads are done by giving the 64-bit logical offset of a message and an <i>S</i>-byte max chunk size. This will return an iterator over the messages contained in the <i>S</i>-byte buffer. <i>S</i> is intended to be larger than any single message, but in the event of an abnormally large message, the read can be retried multiple times, each time doubling the buffer size, until the message is read successfully. A maximum message and buffer size can be specified to make the server reject messages larger than some size, and to give a bound to the client on the maximum it needs to ever read to get a complete message. It is likely that the read buffer ends with a partial message, this is easily detected by the size delimiting.-->
<!--    </p>-->
<!--    <p>-->
<!--        The actual process of reading from an offset requires first locating the log segment file in which the data is stored, calculating the file-specific offset from the global offset value, and then reading from that file offset. The search is done as a simple binary search variation against an in-memory range maintained for each file.-->
<!--    </p>-->
<!--    <p>-->
<!--        The log provides the capability of getting the most recently written message to allow clients to start subscribing as of "right now". This is also useful in the case the consumer fails to consume its data within its SLA-specified number of days. In this case when the client attempts to consume a non-existent offset it is given an OutOfRangeException and can either reset itself or fail as appropriate to the use case.-->
<!--    </p>-->

<!--    <p> The following is the format of the results sent to the consumer.-->

<!--    <pre class="brush: text;">-->
<!--    MessageSetSend (fetch result)-->

<!--    total length     : 4 bytes-->
<!--    error code       : 2 bytes-->
<!--    message 1        : x bytes-->
<!--    ...-->
<!--    message n        : x bytes-->
<!--    </pre>-->

<!--    <pre class="brush: text;">-->
<!--    MultiMessageSetSend (multiFetch result)-->

<!--    total length       : 4 bytes-->
<!--    error code         : 2 bytes-->
<!--    messageSetSend 1-->
<!--    ...-->
<!--    messageSetSend n-->
<!--    </pre>-->
<!--    <h4><a id="impl_deletes" href="#impl_deletes">Deletes</a></h4>-->
<!--    <p>-->
<!--        Data is deleted one log segment at a time. The log manager allows pluggable delete policies to choose which files are eligible for deletion. The current policy deletes any log with a modification time of more than <i>N</i> days ago, though a policy which retained the last <i>N</i> GB could also be useful. To avoid locking reads while still allowing deletes that modify the segment list we use a copy-on-write style segment list implementation that provides consistent views to allow a binary search to proceed on an immutable static snapshot view of the log segments while deletes are progressing.-->
<!--    </p>-->
<!--    <h4><a id="impl_guarantees" href="#impl_guarantees">Guarantees</a></h4>-->
<!--    <p>-->
<!--        The log provides a configuration parameter <i>M</i> which controls the maximum number of messages that are written before forcing a flush to disk. On startup a log recovery process is run that iterates over all messages in the newest log segment and verifies that each message entry is valid. A message entry is valid if the sum of its size and offset are less than the length of the file AND the CRC32 of the message payload matches the CRC stored with the message. In the event corruption is detected the log is truncated to the last valid offset.-->
<!--    </p>-->
<!--    <p>-->
<!--        Note that two kinds of corruption must be handled: truncation in which an unwritten block is lost due to a crash, and corruption in which a nonsense block is ADDED to the file. The reason for this is that in general the OS makes no guarantee of the write order between the file inode and the actual block data so in addition to losing written data the file can gain nonsense data if the inode is updated with a new size but a crash occurs before the block containing that data is written. The CRC detects this corner case, and prevents it from corrupting the log (though the unwritten messages are, of course, lost).-->
<!--    </p>-->

<!--    <h3><a id="distributionimpl" href="#distributionimpl">5.5 Distribution</a></h3>-->
<!--    <h4><a id="impl_offsettracking" href="#impl_offsettracking">Consumer Offset Tracking</a></h4>-->
<!--    <p>-->
<!--        Kafka consumer tracks the maximum offset it has consumed in each partition and has the capability to commit offsets so-->
<!--        that it can resume from those offsets in the event of a restart. Kafka provides the option to store all the offsets for-->
<!--        a given consumer group in a designated broker (for that group) called the group coordinator.  i.e., any consumer instance-->
<!--        in that consumer group should send its offset commits and fetches to that group coordinator (broker). Consumer groups are-->
<!--        assigned to coordinators based on their group names. A consumer can look up its coordinator by issuing a FindCoordinatorRequest-->
<!--        to any Kafka broker and reading the FindCoordinatorResponse which will contain the coordinator details. The consumer-->
<!--        can then proceed to commit or fetch offsets from the coordinator broker. In case the coordinator moves, the consumer will-->
<!--        need to rediscover the coordinator. Offset commits can be done automatically or manually by consumer instance.-->
<!--    </p>-->

<!--    <p>-->
<!--        When the group coordinator receives an OffsetCommitRequest, it appends the request to a special <a href="#compaction">compacted</a> Kafka topic named <i>__consumer_offsets</i>.-->
<!--        The broker sends a successful offset commit response to the consumer only after all the replicas of the offsets topic receive the offsets.-->
<!--        In case the offsets fail to replicate within a configurable timeout, the offset commit will fail and the consumer may retry the commit after backing off.-->
<!--        The brokers periodically compact the offsets topic since it only needs to maintain the most recent offset commit per partition.-->
<!--        The coordinator also caches the offsets in an in-memory table in order to serve offset fetches quickly.-->
<!--    </p>-->

<!--    <p>-->
<!--        When the coordinator receives an offset fetch request, it simply returns the last committed offset vector from the offsets cache.-->
<!--        In case coordinator was just started or if it just became the coordinator for a new set of consumer groups (by becoming a leader for a partition of the offsets topic),-->
<!--        it may need to load the offsets topic partition into the cache. In this case, the offset fetch will fail with an-->
<!--        CoordinatorLoadInProgressException and the consumer may retry the OffsetFetchRequest after backing off.-->
<!--    </p>-->

<!--    <h4><a id="impl_zookeeper" href="#impl_zookeeper">ZooKeeper Directories</a></h4>-->
<!--    <p>-->
<!--        The following gives the ZooKeeper structures and algorithms used for co-ordination between consumers and brokers.-->
<!--    </p>-->

<!--    <h4><a id="impl_zknotation" href="#impl_zknotation">Notation</a></h4>-->
<!--    <p>-->
<!--        When an element in a path is denoted [xyz], that means that the value of xyz is not fixed and there is in fact a ZooKeeper znode for each possible value of xyz. For example /topics/[topic] would be a directory named /topics containing a sub-directory for each topic name. Numerical ranges are also given such as [0...5] to indicate the subdirectories 0, 1, 2, 3, 4. An arrow -> is used to indicate the contents of a znode. For example /hello -> world would indicate a znode /hello containing the value "world".-->
<!--    </p>-->

<!--    <h4><a id="impl_zkbroker" href="#impl_zkbroker">Broker Node Registry</a></h4>-->
<!--    <pre class="brush: json;">-->
<!--    /brokers/ids/[0...N] &ndash;&gt; {"jmx_port":...,"timestamp":...,"endpoints":[...],"host":...,"version":...,"port":...} (ephemeral node)-->
<!--    </pre>-->
<!--    <p>-->
<!--        This is a list of all present broker nodes, each of which provides a unique logical broker id which identifies it to consumers (which must be given as part of its configuration). On startup, a broker node registers itself by creating a znode with the logical broker id under /brokers/ids. The purpose of the logical broker id is to allow a broker to be moved to a different physical machine without affecting consumers. An attempt to register a broker id that is already in use (say because two servers are configured with the same broker id) results in an error.-->
<!--    </p>-->
<!--    <p>-->
<!--        Since the broker registers itself in ZooKeeper using ephemeral znodes, this registration is dynamic and will disappear if the broker is shutdown or dies (thus notifying consumers it is no longer available).-->
<!--    </p>-->
<!--    <h4><a id="impl_zktopic" href="#impl_zktopic">Broker Topic Registry</a></h4>-->
<!--    <pre class="brush: json;">-->
<!--    /brokers/topics/[topic]/partitions/[0...N]/state &ndash;&gt; {"controller_epoch":...,"leader":...,"version":...,"leader_epoch":...,"isr":[...]} (ephemeral node)-->
<!--    </pre>-->

<!--    <p>-->
<!--        Each broker registers itself under the topics it maintains and stores the number of partitions for that topic.-->
<!--    </p>-->

<!--    <h4><a id="impl_clusterid" href="#impl_clusterid">Cluster Id</a></h4>-->

<!--    <p>-->
<!--        The cluster id is a unique and immutable identifier assigned to a Kafka cluster. The cluster id can have a maximum of 22 characters and the allowed characters are defined by the regular expression [a-zA-Z0-9_\-]+, which corresponds to the characters used by the URL-safe Base64 variant with no padding. Conceptually, it is auto-generated when a cluster is started for the first time.-->
<!--    </p>-->
<!--    <p>-->
<!--        Implementation-wise, it is generated when a broker with version 0.10.1 or later is successfully started for the first time. The broker tries to get the cluster id from the <code>/cluster/id</code> znode during startup. If the znode does not exist, the broker generates a new cluster id and creates the znode with this cluster id.-->
<!--    </p>-->

<!--    <h4><a id="impl_brokerregistration" href="#impl_brokerregistration">Broker node registration</a></h4>-->

<!--    <p>-->
<!--        The broker nodes are basically independent, so they only publish information about what they have. When a broker joins, it registers itself under the broker node registry directory and writes information about its host name and port. The broker also register the list of existing topics and their logical partitions in the broker topic registry. New topics are registered dynamically when they are created on the broker.-->
<!--    </p>-->


<!--<div class="p-implementation"></div>-->


<!--<p>Kafka mmaps index files into memory, and all the read / write operations of the index is through OS page cache. This-->
<!--avoids blocked disk I/O in most cases.</p>-->

<!--<p>To the extent of our knowledge, all the modern operating systems use LRU policy or its variants to manage page-->
<!--cache. Kafka always appends to the end of the index file, and almost all the index lookups (typically from in-sync-->
<!--followers or consumers) are very close to the end of the index. So, the LRU cache replacement policy should work very-->
<!--well with Kafka's index access pattern.</p>-->

<!--<p>However, when looking up index, the standard binary search algorithm is not cache friendly, and can cause unnecessary-->
<!--page faults (the thread is blocked to wait for reading some index entries from hard disk, as those entries are not-->
<!--cached in the page cache).</p>-->

<!--<p>For example, in an index with 13 pages, to lookup an entry in the last page (page #12), the standard binary search-->
<!--algorithm will read index entries in page #0, 6, 9, 11, and 12.</p>-->
<!--page number: |0|1|2|3|4|5|6|7|8|9|10|11|12 | <br>-->
<!--steps:       |1| | | | | |3| | |4|  |5 |2/6| <br>-->
<!--In each page, there are hundreds log entries, corresponding to hundreds to thousands of kafka messages. When the-->
<!--index gradually growing from the 1st entry in page #12 to the last entry in page #12, all the write (append)-->
<!--operations are in page #12, and all the in-sync follower / consumer lookups read page #0,6,9,11,12. As these pages-->
<!--are always used in each in-sync lookup, we can assume these pages are fairly recently used, and are very likely to be-->
<!--in the page cache. When the index grows to page #13, the pages needed in a in-sync lookup change to #0, 7, 10, 12,-->
<!--and 13:<br>-->
<!--page number: |0|1|2|3|4|5|6|7|8|9|10|11|12|13 |<br>-->
<!--steps:       |1| | | | | | |3| | | 4|5 | 6|2/7|<br>-->
<!--Page #7 and page #10 have not been used for a very long time. They are much less likely to be in the page cache, than-->
<!--the other pages. The 1st lookup, after the 1st index entry in page #13 is appended, is likely to have to read page #7-->
<!--and page #10 from disk (page fault), which can take up to more than a second. In our test, this can cause the-->
<!--at-least-once produce latency to jump to about 1 second from a few ms.<br>-->

<!--Here, we use a more cache-friendly lookup algorithm:<br>-->
<!--if (target > indexEntry[end - N]) // if the target is in the last N entries of the index<br>-->
<!--binarySearch(end - N, end)<br>-->
<!--else<br>-->
<!--binarySearch(begin, end - N)<br>-->

<!--If possible, we only look up in the last N entries of the index. By choosing a proper constant N, all the in-sync-->
<!--lookups should go to the 1st branch. We call the last N entries the "warm" section. As we frequently look up in this-->
<!--relatively small section, the pages containing this section are more likely to be in the page cache.<br>-->

<!--We set N (_warmEntries) to 8192, because-->
<!--1. This number is small enough to guarantee all the pages of the "warm" section is touched in every warm-section-->
<!--lookup. So that, the entire warm section is really "warm".-->
<!--When doing warm-section lookup, following 3 entries are always touched: indexEntry(end), indexEntry(end-N),-->
<!--and indexEntry((end*2 -N)/2). If page size >= 4096, all the warm-section pages (3 or fewer) are touched, when we-->
<!--touch those 3 entries. As of 2018, 4096 is the smallest page size for all the processors (x86-32, x86-64, MIPS,-->
<!--SPARC, Power, ARM etc.).<br>-->
<!--2. This number is large enough to guarantee most of the in-sync lookups are in the warm-section. With default Kafka-->
<!--settings, 8KB index corresponds to about 4MB (offset index) or 2.7MB (time index) log messages.<br>-->

<!--We can't set make N (_warmEntries) to be larger than 8192, as there is no simple way to guarantee all the "warm"-->
<!--section pages are really warm (touched in every lookup) on a typical 4KB-page host.<br>-->

<!--In there future, we may use a backend thread to periodically touch the entire warm section. So that, we can<br>-->
<!--1) support larger warm section<br>-->
<!--2) make sure the warm section of low QPS topic-partitions are really warm.<br>-->
<!--The cleaner is responsible for removing obsolete records from logs which have the "compact" retention strategy.-->
<!--A message with key K and offset O is obsolete if there exists a message with key K and offset O' such that O < O'.<br>-->

<!--Each log can be thought of being split into two sections of segments: a "clean" section which has previously been cleaned followed by a-->
<!--"dirty" section that has not yet been cleaned. The dirty section is further divided into the "cleanable" section followed by an "uncleanable" section.-->
<!--The uncleanable section is excluded from cleaning. The active log segment is always uncleanable. If there is a-->
<!--compaction lag time set, segments whose largest message timestamp is within the compaction lag time of the cleaning operation are also uncleanable.-->
<!--<br>-->
<!--The cleaning is carried out by a pool of background threads. Each thread chooses the dirtiest log that has the "compact" retention policy-->
<!--and cleans that. The dirtiness of the log is guessed by taking the ratio of bytes in the dirty section of the log to the total bytes in the log.-->
<!--<br>-->
<!--To clean a log the cleaner first builds a mapping of key=>last_offset for the dirty section of the log. See kafka.log.OffsetMap for details of-->
<!--the implementation of the mapping.-->
<!--<br>-->
<!--Once the key=>last_offset map is built, the log is cleaned by recopying each log segment but omitting any key that appears in the offset map with a-->
<!--higher offset than what is found in the segment (i.e. messages with a key that appears in the dirty section of the log).-->
<!--<br>-->
<!--To avoid segments shrinking to very small sizes with repeated cleanings we implement a rule by which if we will merge successive segments when-->
<!--doing a cleaning if their log and index size are less than the maximum log and index size prior to the clean beginning.-->
<!--<br>-->
<!--Cleaned segments are swapped into the log as they become available.-->
<!--<br>-->
<!--One nuance that the cleaner must handle is log truncation. If a log is truncated while it is being cleaned the cleaning of that log is aborted.-->
<!--<br>-->
<!--Messages with null payload are treated as deletes for the purpose of log compaction. This means that they receive special treatment by the cleaner.-->
<!--The cleaner will only retain delete records for a period of time to avoid accumulating space indefinitely. This period of time is configurable on a per-topic-->
<!--basis and is measured from the time the segment enters the clean portion of the log (at which point any prior message with that key has been removed).-->
<!--Delete markers in the clean section of the log that are older than this time will not be retained when log segments are being recopied as part of cleaning.-->
<!--<br>-->
<!--Note that cleaning is more complicated with the idempotent/transactional producer capabilities. The following-->
<!--are the key points:-->
<!--<br>-->
<!--1. In order to maintain sequence number continuity for active producers, we always retain the last batch-->
<!--from each producerId, even if all the records from the batch have been removed. The batch will be removed-->
<!--once the producer either writes a new batch or is expired due to inactivity.-->
<!--2. We do not clean beyond the last stable offset. This ensures that all records observed by the cleaner have-->
<!--been decided (i.e. committed or aborted). In particular, this allows us to use the transaction index to-->
<!--collect the aborted transactions ahead of time.-->
<!--3. Records from aborted transactions are removed by the cleaner immediately without regard to record keys.-->
<!--4. Transaction markers are retained until all record batches from the same transaction have been removed and-->
<!--a sufficient amount of time has passed to reasonably ensure that an active consumer wouldn't consume any-->
<!--data from the transaction prior to reaching the offset of the marker. This follows the same logic used for-->
<!--tombstone deletion.-->
<!--<br>-->

 Hierarchical Timing Wheels<br>
<br>
 A simple timing wheel is a circular list of buckets of timer tasks. Let u be the time unit.<br>
 A timing wheel with size n has n buckets and can hold timer tasks in n * u time interval.<br>
 Each bucket holds timer tasks that fall into the corresponding time range. At the beginning,<br>
 the first bucket holds tasks for [0, u), the second bucket holds tasks for [u, 2u), …,<br>
 the n-th bucket for [u * (n -1), u * n). Every interval of time unit u, the timer ticks and<br>
 moved to the next bucket then expire all timer tasks in it. So, the timer never insert a task<br>
 into the bucket for the current time since it is already expired. The timer immediately runs<br>
 the expired task. The emptied bucket is then available for the next round, so if the current<br>
 bucket is for the time t, it becomes the bucket for [t + u * n, t + (n + 1) * u) after a tick.<br>
 A timing wheel has O(1) cost for insert/delete (start-timer/stop-timer) whereas priority queue<br>
 based timers, such as java.util.concurrent.DelayQueue and java.util.Timer, have O(log n)<br>
 insert/delete cost.<br>

 A major drawback of a simple timing wheel is that it assumes that a timer request is within<br>
 the time interval of n * u from the current time. If a timer request is out of this interval,<br>
 it is an overflow. A hierarchical timing wheel deals with such overflows. It is a hierarchically<br>
 organized timing wheels. The lowest level has the finest time resolution. As moving up the<br>
 hierarchy, time resolutions become coarser. If the resolution of a wheel at one level is u and<br>
 the size is n, the resolution of the next level should be n * u. At each level overflows are<br>
 delegated to the wheel in one level higher. When the wheel in the higher level ticks, it reinsert<br>
 timer tasks to the lower level. An overflow wheel can be created on-demand. When a bucket in an<br>
 overflow bucket expires, all tasks in it are reinserted into the timer recursively. The tasks<br>
 are then moved to the finer grain wheels or be executed. The insert (start-timer) cost is O(m)<br>
 where m is the number of wheels, which is usually very small compared to the number of requests<br>
 in the system, and the delete (stop-timer) cost is still O(1).<br>
<br>
 Example<br>
 Let's say that u is 1 and n is 3. If the start time is c,<br>
 then the buckets at different levels are:<br>
<br>
 level    buckets<br>
 1        [c,c]   [c+1,c+1]  [c+2,c+2]<br>
 2        [c,c+2] [c+3,c+5]  [c+6,c+8]<br>
 3        [c,c+8] [c+9,c+17] [c+18,c+26]<br>
<br>
 The bucket expiration is at the time of bucket beginning.<br>
 So at time = c+1, buckets [c,c], [c,c+2] and [c,c+8] are expired.<br>
 Level 1's clock moves to c+1, and [c+3,c+3] is created.<br>
 Level 2 and level3's clock stay at c since their clocks move in unit of 3 and 9, respectively.<br>
 So, no new buckets are created in level 2 and 3.<br>
<br>
 Note that bucket [c,c+2] in level 2 won't receive any task since that range is already covered in level 1.<br>
 The same is true for the bucket [c,c+8] in level 3 since its range is covered in level 2.<br>
 This is a bit wasteful, but simplifies the implementation.<br>
<br>
 1        [c+1,c+1]  [c+2,c+2]  [c+3,c+3]<br>
 2        [c,c+2]    [c+3,c+5]  [c+6,c+8]<br>
 3        [c,c+8]    [c+9,c+17] [c+18,c+26]<br>
<br>
 At time = c+2, [c+1,c+1] is newly expired.<br>
 Level 1 moves to c+2, and [c+4,c+4] is created,<br>
<br>
 1        [c+2,c+2]  [c+3,c+3]  [c+4,c+4]<br>
 2        [c,c+2]    [c+3,c+5]  [c+6,c+8]<br>
 3        [c,c+8]    [c+9,c+17] [c+18,c+18]<br>
<br>
 At time = c+3, [c+2,c+2] is newly expired.<br>
 Level 2 moves to c+3, and [c+5,c+5] and [c+9,c+11] are created.<br>
 Level 3 stay at c.<br>
<br>
 1        [c+3,c+3]  [c+4,c+4]  [c+5,c+5]<br>
 2        [c+3,c+5]  [c+6,c+8]  [c+9,c+11]<br>
 3        [c,c+8]    [c+9,c+17] [c+8,c+11]<br>
<br>
 The hierarchical timing wheels works especially well when operations are completed before they time out.<br>
 Even when everything times out, it still has advantageous when there are many items in the timer.<br>
 Its insert cost (including reinsert) and delete cost are O(m) and O(1), respectively while priority<br>
 queue based timers takes O(log N) for both insert and delete where N is the number of items in the queue.<br>
<br>
 This class is not thread-safe. There should not be any add calls while advanceClock is executing.<br>
 It is caller's responsibility to enforce it. Simultaneous add calls are thread-safe.
<br>
The bucket needs to be enqueued because it was an expired bucket<br>
We only need to enqueue the bucket when its expiration time has changed, i.e. the wheel has advanced
and the previous buckets gets reused; further calls to set the expiration within the same wheel cycle
will pass in the same value and hence return false, thus the bucket with the same expiration will not
be enqueued multiple times.<br>


























-------------------------------------------<br>
* This class manages the state of each partition being cleaned.<br>
* LogCleaningState defines the cleaning states that a TopicPartition can be in.<br>
* 1. None                    : No cleaning state in a TopicPartition. In this state, it can become LogCleaningInProgress<br>
*                              or LogCleaningPaused(1). Valid previous state are LogCleaningInProgress and LogCleaningPaused(1)<br>
* 2. LogCleaningInProgress   : The cleaning is currently in progress. In this state, it can become None when log cleaning is finished<br>
*                              or become LogCleaningAborted. Valid previous state is None.<br>
* 3. LogCleaningAborted      : The cleaning abort is requested. In this state, it can become LogCleaningPaused(1).<br>
*                              Valid previous state is LogCleaningInProgress.<br>
* 4-a. LogCleaningPaused(1)  : The cleaning is paused once. No log cleaning can be done in this state.<br>
*                              In this state, it can become None or LogCleaningPaused(2).<br>
*                              Valid previous state is None, LogCleaningAborted or LogCleaningPaused(2).<br>
* 4-b. LogCleaningPaused(i)  : The cleaning is paused i times where i>= 2. No log cleaning can be done in this state.<br>
*                              In this state, it can become LogCleaningPaused(i-1) or LogCleaningPaused(i+1).<br>
*                              Valid previous state is LogCleaningPaused(i-1) or LogCleaningPaused(i+1).<br>




An operation whose processing needs to be delayed for at most the given delayMs. For example<br>
a delayed produce operation could be waiting for specified number of acks;<br> or
a delayed fetch operation could be waiting for a given number of bytes to accumulate.<br>
<br>
The logic upon completing a delayed operation is defined in onComplete() and will be called exactly once. <br>
Once an operation is completed, isCompleted() will return true.<br> onComplete() can be triggered by either
forceComplete(), which forces calling onComplete() after delayMs if the operation is not yet completed,
or tryComplete(), which first checks if the operation can be completed or not now, and if yes calls
forceComplete().<br>
A subclass of DelayedOperation needs to provide an implementation of both onComplete() and tryComplete().<br>

 Force completing the delayed operation, if not already completed.<br>
This function can be triggered when<br>
<br>
1. The operation has been verified to be completable inside tryComplete()<br>
2. The operation has expired and hence needs to be completed right now<br>
<br>
Return true iff the operation is completed by the caller: note that
concurrent threads can try to complete the same operation, but only
the first thread will succeed in completing the operation and return
true, others will still return false<br>
Call-back to execute when a delayed operation gets expired and hence forced to complete.<br>
Process for completing an operation; This function needs to be defined<br>
in subclasses and will be called exactly once in forceComplete()<br>


Try to complete the delayed operation by first checking if the operation<br>
can be completed by now. If yes execute the completion logic by calling<br>
forceComplete() and return true iff forceComplete returns true; otherwise return false<br>

This function needs to be defined in subclasses<br>

Thread-safe variant of tryComplete() that attempts completion only if the lock can be acquired
without blocking.<br>

If threadA acquires the lock and performs the check for completion before completion criteria is met
and threadB satisfies the completion criteria, but fails to acquire the lock because threadA has not
yet released the lock, we need to ensure that completion is attempted again without blocking threadA
or threadB. <br>`tryCompletePending` is set by threadB when it fails to acquire the lock and at least one
of threadA or threadB will attempt completion of the operation if this flag is set. <br>This ensures that
every invocation of `maybeTryComplete` is followed by at least one invocation of `tryComplete` until
the operation is actually completed.<br>
<br>
<br>
<br>
While we were holding the lock, another thread may have invoked `maybeTryComplete` and set
`tryCompletePending`. In this case we should retry.<br>
<br>
<br>
<br>
Another thread is holding the lock. If `tryCompletePending` is already set and this thread failed to
acquire the lock, then the thread that is holding the lock is guaranteed to see the flag and retry.<br>
Otherwise, we should set the flag and retry on this thread since the thread holding the lock may have
released the lock and returned by the time the flag is set.<br>

<br>
<br>
<br>
run() method defines a task that is executed on timeout<br>



--------------------------------------<br>
Check if the operation can be completed, if not watch it based on the given watch keys<br>
<br>
Note that a delayed operation can be watched on multiple keys.<br> It is possible that
an operation is completed after it has been added to the watch list for some, but
not all of the keys. <br>In this case, the operation is considered completed and won't
be added to the watch list of the remaining keys. <br>The expiration reaper thread will
remove this operation from any watcher list in which the operation exists.<br>
<br>
<br>
<br>
<br>
The cost of tryComplete() is typically proportional to the number of keys. <br>Calling
tryComplete() for each key is going to be expensive if there are many keys. <br>Instead,
we do the check in the following way.   Call tryComplete(). If the operation is not completed,
we just add the operation to all keys. Then we call tryComplete() again. <br>At this time, if
the operation is still not completed, we are guaranteed that it won't miss any future triggering
event since the operation is already on the watcher list for all keys. <br>This does mean that
if the operation is completed (by another thread) between the two tryComplete() calls, the
operation is unnecessarily added for watch. However, this is a less severe issue since the
expire reaper will clean it up periodically.<br>